{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modelrnnlstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLvGsZmg2mFX",
        "colab_type": "code",
        "outputId": "e7860420-8322-449b-be65-e32e3cd5612f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv6Yghq42_iH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from string import digits\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import re\n",
        "#from sklearn.cross_validation import train_test_split\n",
        "# Building a english to french translator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mFjla063cdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines= pd.read_table('/content/drive/My Drive/NLPProject/part10', names=['questions', 'keywords'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRRo9mtb39dp",
        "colab_type": "code",
        "outputId": "03a6178a-e34a-4f1e-872d-c28b5ddc1c45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "#lines = lines[0:50000]\n",
        "print(type(lines.keywords))\n",
        "print(lines.keywords)\n",
        "lines.keywords = lines.keywords.apply(lambda keywrds: \" \".join([ele for ele in eval(keywrds)]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "0            ['Replace', 'battery', 'key', 'jetta', 'gls']\n",
            "1             ['Replace', 'battery', 'VW', 'jetta', 'key']\n",
            "2         ['Replace', 'battery', 'VW', 'jetta', 'key', ...\n",
            "3                              ['Replace', 'key', 'jetta']\n",
            "4         ['Replace', 'keyless', 'remote', 'battery', '...\n",
            "                               ...                        \n",
            "12283              ['does', 'mountain', 'effect', 'earth']\n",
            "12284     ['does', 'mountains', 'effect', 'earths', 'su...\n",
            "12285     ['does', 'mountains', 'effect', 'earth', 'sur...\n",
            "12286                      ['does', 'mountains', 'effect']\n",
            "12287       ['does', 'rain', 'effect', 'earth', 'surface']\n",
            "Name: keywords, Length: 12288, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jFNHnBn3_vN",
        "colab_type": "code",
        "outputId": "6ca4d14a-a7df-4cca-980f-d411f6c0f856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lines.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12288, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h2SjINj4BgX",
        "colab_type": "code",
        "outputId": "e45ff8f5-5a24-466a-8a39-3dbd1dc22471",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "lines.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>questions</th>\n",
              "      <th>keywords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Replace battery in key of 2003 jetta gls?</td>\n",
              "      <td>Replace battery key jetta gls</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Replace battery on 2006 VW jetta key?</td>\n",
              "      <td>Replace battery VW jetta key</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Replace battery on VW jetta 2001 key remote?</td>\n",
              "      <td>Replace battery VW jetta key remote</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Replace key 1999 jetta?</td>\n",
              "      <td>Replace key jetta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Replace keyless remote battery on volkswagen j...</td>\n",
              "      <td>Replace keyless remote battery volkswagen jetta</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           questions                                         keywords\n",
              "0         Replace battery in key of 2003 jetta gls?                     Replace battery key jetta gls\n",
              "1             Replace battery on 2006 VW jetta key?                      Replace battery VW jetta key\n",
              "2      Replace battery on VW jetta 2001 key remote?               Replace battery VW jetta key remote\n",
              "3                           Replace key 1999 jetta?                                 Replace key jetta\n",
              "4  Replace keyless remote battery on volkswagen j...  Replace keyless remote battery volkswagen jetta"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMFDq4PX4DVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines.questions=lines.questions.apply(lambda x: x.lower())\n",
        "lines.keywords=lines.keywords.apply(lambda x: x.lower())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8uV1Tov4GZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#removing the apostraphys and replacing comma(,) with COMMA\n",
        "lines.questions=lines.questions.apply(lambda x: re.sub(\"'\", '', x)).apply(lambda x: re.sub(\",\", ' COMMA', x))\n",
        "lines.keywords=lines.keywords.apply(lambda x: re.sub(\"'\", '', x)).apply(lambda x: re.sub(\",\", ' COMMA', x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cnU5paX4IqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#removing the punctuation\n",
        "punctu = set(string.punctuation)\n",
        "lines.questions=lines.questions.apply(lambda x: ''.join(ch for ch in x if ch not in punctu))\n",
        "lines.keywords=lines.keywords.apply(lambda x: ''.join(ch for ch in x if ch not in punctu))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_064RhTi4KqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#removing the digits\n",
        "dig = str.maketrans('', '', digits)\n",
        "lines.questions=lines.questions.apply(lambda x: x.translate(dig))\n",
        "lines.keywords=lines.keywords.apply(lambda x: x.translate(dig))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfuaQOAG4NAO",
        "colab_type": "code",
        "outputId": "5d3a2340-2122-41a6-b817-72974922df0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "lines.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>questions</th>\n",
              "      <th>keywords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>replace battery in key of  jetta gls</td>\n",
              "      <td>replace battery key jetta gls</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>replace battery on  vw jetta key</td>\n",
              "      <td>replace battery vw jetta key</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>replace battery on vw jetta  key remote</td>\n",
              "      <td>replace battery vw jetta key remote</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>replace key  jetta</td>\n",
              "      <td>replace key jetta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>replace keyless remote battery on volkswagen j...</td>\n",
              "      <td>replace keyless remote battery volkswagen jetta</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           questions                                         keywords\n",
              "0              replace battery in key of  jetta gls                     replace battery key jetta gls\n",
              "1                  replace battery on  vw jetta key                      replace battery vw jetta key\n",
              "2           replace battery on vw jetta  key remote               replace battery vw jetta key remote\n",
              "3                                replace key  jetta                                 replace key jetta\n",
              "4  replace keyless remote battery on volkswagen j...  replace keyless remote battery volkswagen jetta"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7ZO1bcZ4OYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines.questions = lines.questions.apply(lambda x : 'START_ '+ x + ' _END')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UScFS8F4Q3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_ques_words = set()\n",
        "for ques in lines.questions:\n",
        "    for word in ques.split():\n",
        "        if word not in all_ques_words:\n",
        "            all_ques_words.add(word)\n",
        "    \n",
        "all_key_words = set()\n",
        "for k in lines.keywords:\n",
        "    for word in k.split():\n",
        "        if word not in all_key_words:\n",
        "            all_key_words.add(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGLIXOUK4TSL",
        "colab_type": "code",
        "outputId": "19df9d81-5b47-43fe-e94c-ff6740a796ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(all_ques_words), len(all_key_words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7739, 7220)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCHJ6TZt4WJs",
        "colab_type": "code",
        "outputId": "242ee935-ecc3-4bf9-b5a9-338c49ca906c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lenght_list=[]\n",
        "for l in lines.questions:\n",
        "    lenght_list.append(len(l.split(' ')))\n",
        "maxqueslength = np.max(lenght_list)\n",
        "maxqueslength"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZ0LNrXo4YFb",
        "colab_type": "code",
        "outputId": "4ca19a7e-bf92-4a84-f7fc-ad1a9115db5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lenght_list=[]\n",
        "for l in lines.keywords:\n",
        "    lenght_list.append(len(l.split(' ')))\n",
        "maxkeylength = np.max(lenght_list)\n",
        "maxkeylength"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVN9MnVu4c84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_words = sorted(list(all_key_words))\n",
        "target_words = sorted(list(all_ques_words))\n",
        "num_encoder_tokens = len(all_key_words)\n",
        "num_decoder_tokens = len(all_ques_words)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_84JqCC4fEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_token_index = dict(\n",
        "    [(word, i) for i, word in enumerate(input_words)])\n",
        "target_token_index = dict(\n",
        "    [(word, i) for i, word in enumerate(target_words)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wupHEAsK4hNE",
        "colab_type": "code",
        "outputId": "37198a4b-60cc-4090-a2e3-a48a729256c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(lines.questions)*maxqueslength*num_decoder_tokens"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4279357440"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsbjuPAP4rjj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_input_data = np.zeros(\n",
        "    (len(lines.keywords), maxkeylength),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(lines.questions), maxqueslength),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(lines.questions), maxqueslength, num_decoder_tokens),\n",
        "    dtype='float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5x1U0fS4uFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, (input_text, target_text) in enumerate(zip(lines.keywords, lines.questions)):\n",
        "    for t, word in enumerate(input_text.split()):\n",
        "        encoder_input_data[i, t] = input_token_index[word]\n",
        "    for t, word in enumerate(target_text.split()):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t] = target_token_index[word]\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[word]] = 1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2BJTI2M46Qm",
        "colab_type": "text"
      },
      "source": [
        "### Build keras encoder-decoder model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C87qj_aK40n5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_size = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPt7xDw845IM",
        "colab_type": "code",
        "outputId": "5906caa9-1260-4b89-e1f8-ef650d29b4b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "from keras.layers import Input, LSTM, Embedding, Dense\n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model\n",
        "from keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELhzpZzJ5X5K",
        "colab_type": "text"
      },
      "source": [
        "#### Encoder model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC33G8HR5CVC",
        "colab_type": "code",
        "outputId": "03588dae-3e1c-437e-c5a4-caede050f7cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "encoder_inputs = Input(shape=(None,))\n",
        "en_x=  Embedding(num_encoder_tokens, embedding_size)(encoder_inputs)\n",
        "encoder = LSTM(units=50, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(en_x)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5-5S_wc5qMr",
        "colab_type": "text"
      },
      "source": [
        "#### Decoder model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHPD37QN5j9g",
        "colab_type": "code",
        "outputId": "c50b0cdf-c9a0-4ed1-fd2a-6225afd8aba4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "dex=  Embedding(num_decoder_tokens, embedding_size)\n",
        "\n",
        "final_dex= dex(decoder_inputs)\n",
        "\n",
        "\n",
        "decoder_lstm = LSTM(units=50, return_sequences=True, return_state=True)\n",
        "\n",
        "decoder_outputs, _, _ = decoder_lstm(final_dex,\n",
        "                                     initial_state=encoder_states)\n",
        "\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "import os.path\n",
        "from os import path\n",
        "model = \"\"\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iiujj2h53CR",
        "colab_type": "code",
        "outputId": "843e4d7c-77ec-4753-db9c-f99b33ba5251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 50)     361000      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, None, 50)     386950      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 50), (None,  20200       embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, None, 50), ( 20200       embedding_2[0][0]                \n",
            "                                                                 lstm_1[0][1]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 7739)   394689      lstm_2[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 1,183,039\n",
            "Trainable params: 1,183,039\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6xc7Prp57Ii",
        "colab_type": "code",
        "outputId": "d9c7b14c-f8ef-4cbd-e751-cb2f52b12a7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=128,\n",
        "          epochs=300,\n",
        "          validation_split=0.05 )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11673 samples, validate on 615 samples\n",
            "Epoch 1/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 1.4768 - acc: 0.0285 - val_loss: 1.3850 - val_acc: 0.0297\n",
            "Epoch 2/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 1.2961 - acc: 0.0298 - val_loss: 1.4051 - val_acc: 0.0297\n",
            "Epoch 3/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 1.2836 - acc: 0.0313 - val_loss: 1.4158 - val_acc: 0.0336\n",
            "Epoch 4/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 1.2748 - acc: 0.0330 - val_loss: 1.4209 - val_acc: 0.0339\n",
            "Epoch 5/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 1.2666 - acc: 0.0337 - val_loss: 1.4279 - val_acc: 0.0308\n",
            "Epoch 6/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 1.2591 - acc: 0.0346 - val_loss: 1.4330 - val_acc: 0.0343\n",
            "Epoch 7/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 1.2518 - acc: 0.0353 - val_loss: 1.4363 - val_acc: 0.0350\n",
            "Epoch 8/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 1.2456 - acc: 0.0358 - val_loss: 1.4320 - val_acc: 0.0354\n",
            "Epoch 9/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 1.2360 - acc: 0.0363 - val_loss: 1.4171 - val_acc: 0.0353\n",
            "Epoch 10/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 1.2202 - acc: 0.0371 - val_loss: 1.4043 - val_acc: 0.0366\n",
            "Epoch 11/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 1.1967 - acc: 0.0403 - val_loss: 1.3772 - val_acc: 0.0375\n",
            "Epoch 12/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 1.1719 - acc: 0.0440 - val_loss: 1.3696 - val_acc: 0.0404\n",
            "Epoch 13/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 1.1486 - acc: 0.0463 - val_loss: 1.3615 - val_acc: 0.0408\n",
            "Epoch 14/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 1.1272 - acc: 0.0491 - val_loss: 1.3599 - val_acc: 0.0420\n",
            "Epoch 15/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 1.1071 - acc: 0.0512 - val_loss: 1.3561 - val_acc: 0.0430\n",
            "Epoch 16/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 1.0873 - acc: 0.0529 - val_loss: 1.3536 - val_acc: 0.0433\n",
            "Epoch 17/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 1.0671 - acc: 0.0549 - val_loss: 1.3509 - val_acc: 0.0451\n",
            "Epoch 18/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 1.0468 - acc: 0.0570 - val_loss: 1.3473 - val_acc: 0.0448\n",
            "Epoch 19/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 1.0268 - acc: 0.0584 - val_loss: 1.3469 - val_acc: 0.0451\n",
            "Epoch 20/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 1.0082 - acc: 0.0598 - val_loss: 1.3458 - val_acc: 0.0458\n",
            "Epoch 21/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.9904 - acc: 0.0617 - val_loss: 1.3461 - val_acc: 0.0476\n",
            "Epoch 22/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.9731 - acc: 0.0631 - val_loss: 1.3466 - val_acc: 0.0490\n",
            "Epoch 23/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.9568 - acc: 0.0647 - val_loss: 1.3472 - val_acc: 0.0493\n",
            "Epoch 24/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.9392 - acc: 0.0674 - val_loss: 1.3461 - val_acc: 0.0511\n",
            "Epoch 25/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.9212 - acc: 0.0699 - val_loss: 1.3426 - val_acc: 0.0515\n",
            "Epoch 26/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.9042 - acc: 0.0719 - val_loss: 1.3439 - val_acc: 0.0507\n",
            "Epoch 27/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.8871 - acc: 0.0739 - val_loss: 1.3412 - val_acc: 0.0529\n",
            "Epoch 28/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.8710 - acc: 0.0757 - val_loss: 1.3430 - val_acc: 0.0522\n",
            "Epoch 29/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.8551 - acc: 0.0778 - val_loss: 1.3406 - val_acc: 0.0543\n",
            "Epoch 30/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.8394 - acc: 0.0797 - val_loss: 1.3400 - val_acc: 0.0551\n",
            "Epoch 31/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.8239 - acc: 0.0819 - val_loss: 1.3422 - val_acc: 0.0551\n",
            "Epoch 32/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.8090 - acc: 0.0839 - val_loss: 1.3459 - val_acc: 0.0551\n",
            "Epoch 33/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.7941 - acc: 0.0861 - val_loss: 1.3488 - val_acc: 0.0547\n",
            "Epoch 34/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.7798 - acc: 0.0880 - val_loss: 1.3493 - val_acc: 0.0560\n",
            "Epoch 35/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.7662 - acc: 0.0898 - val_loss: 1.3528 - val_acc: 0.0567\n",
            "Epoch 36/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.7517 - acc: 0.0918 - val_loss: 1.3558 - val_acc: 0.0576\n",
            "Epoch 37/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.7385 - acc: 0.0937 - val_loss: 1.3593 - val_acc: 0.0571\n",
            "Epoch 38/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.7259 - acc: 0.0956 - val_loss: 1.3593 - val_acc: 0.0578\n",
            "Epoch 39/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.7130 - acc: 0.0975 - val_loss: 1.3629 - val_acc: 0.0573\n",
            "Epoch 40/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.7002 - acc: 0.0991 - val_loss: 1.3636 - val_acc: 0.0576\n",
            "Epoch 41/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.6883 - acc: 0.1006 - val_loss: 1.3675 - val_acc: 0.0568\n",
            "Epoch 42/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.6773 - acc: 0.1024 - val_loss: 1.3701 - val_acc: 0.0566\n",
            "Epoch 43/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.6659 - acc: 0.1041 - val_loss: 1.3768 - val_acc: 0.0558\n",
            "Epoch 44/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.6552 - acc: 0.1054 - val_loss: 1.3757 - val_acc: 0.0564\n",
            "Epoch 45/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.6449 - acc: 0.1070 - val_loss: 1.3788 - val_acc: 0.0567\n",
            "Epoch 46/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.6347 - acc: 0.1085 - val_loss: 1.3790 - val_acc: 0.0562\n",
            "Epoch 47/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.6247 - acc: 0.1099 - val_loss: 1.3832 - val_acc: 0.0568\n",
            "Epoch 48/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.6154 - acc: 0.1111 - val_loss: 1.3843 - val_acc: 0.0563\n",
            "Epoch 49/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.6059 - acc: 0.1124 - val_loss: 1.3875 - val_acc: 0.0573\n",
            "Epoch 50/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.5969 - acc: 0.1136 - val_loss: 1.3932 - val_acc: 0.0563\n",
            "Epoch 51/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.5879 - acc: 0.1150 - val_loss: 1.3937 - val_acc: 0.0568\n",
            "Epoch 52/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.5796 - acc: 0.1159 - val_loss: 1.3975 - val_acc: 0.0563\n",
            "Epoch 53/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.5713 - acc: 0.1171 - val_loss: 1.3993 - val_acc: 0.0558\n",
            "Epoch 54/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.5632 - acc: 0.1181 - val_loss: 1.4010 - val_acc: 0.0560\n",
            "Epoch 55/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.5552 - acc: 0.1193 - val_loss: 1.4039 - val_acc: 0.0571\n",
            "Epoch 56/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.5472 - acc: 0.1203 - val_loss: 1.4064 - val_acc: 0.0569\n",
            "Epoch 57/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.5399 - acc: 0.1211 - val_loss: 1.4117 - val_acc: 0.0562\n",
            "Epoch 58/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.5324 - acc: 0.1222 - val_loss: 1.4143 - val_acc: 0.0563\n",
            "Epoch 59/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.5250 - acc: 0.1232 - val_loss: 1.4151 - val_acc: 0.0566\n",
            "Epoch 60/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.5178 - acc: 0.1241 - val_loss: 1.4207 - val_acc: 0.0562\n",
            "Epoch 61/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.5108 - acc: 0.1251 - val_loss: 1.4229 - val_acc: 0.0560\n",
            "Epoch 62/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.5042 - acc: 0.1260 - val_loss: 1.4231 - val_acc: 0.0563\n",
            "Epoch 63/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.4976 - acc: 0.1268 - val_loss: 1.4282 - val_acc: 0.0556\n",
            "Epoch 64/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.4910 - acc: 0.1279 - val_loss: 1.4300 - val_acc: 0.0553\n",
            "Epoch 65/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.4844 - acc: 0.1287 - val_loss: 1.4331 - val_acc: 0.0561\n",
            "Epoch 66/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.4783 - acc: 0.1295 - val_loss: 1.4377 - val_acc: 0.0557\n",
            "Epoch 67/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.4725 - acc: 0.1304 - val_loss: 1.4343 - val_acc: 0.0567\n",
            "Epoch 68/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.4660 - acc: 0.1310 - val_loss: 1.4416 - val_acc: 0.0562\n",
            "Epoch 69/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.4602 - acc: 0.1318 - val_loss: 1.4414 - val_acc: 0.0569\n",
            "Epoch 70/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.4546 - acc: 0.1327 - val_loss: 1.4467 - val_acc: 0.0559\n",
            "Epoch 71/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.4485 - acc: 0.1335 - val_loss: 1.4491 - val_acc: 0.0563\n",
            "Epoch 72/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.4428 - acc: 0.1342 - val_loss: 1.4522 - val_acc: 0.0558\n",
            "Epoch 73/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.4373 - acc: 0.1350 - val_loss: 1.4550 - val_acc: 0.0559\n",
            "Epoch 74/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.4324 - acc: 0.1356 - val_loss: 1.4571 - val_acc: 0.0557\n",
            "Epoch 75/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.4271 - acc: 0.1364 - val_loss: 1.4616 - val_acc: 0.0554\n",
            "Epoch 76/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.4218 - acc: 0.1370 - val_loss: 1.4657 - val_acc: 0.0552\n",
            "Epoch 77/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.4169 - acc: 0.1376 - val_loss: 1.4689 - val_acc: 0.0547\n",
            "Epoch 78/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.4120 - acc: 0.1384 - val_loss: 1.4705 - val_acc: 0.0547\n",
            "Epoch 79/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.4066 - acc: 0.1394 - val_loss: 1.4733 - val_acc: 0.0551\n",
            "Epoch 80/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.4020 - acc: 0.1399 - val_loss: 1.4735 - val_acc: 0.0544\n",
            "Epoch 81/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.3970 - acc: 0.1407 - val_loss: 1.4761 - val_acc: 0.0559\n",
            "Epoch 82/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.3922 - acc: 0.1414 - val_loss: 1.4832 - val_acc: 0.0539\n",
            "Epoch 83/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.3881 - acc: 0.1420 - val_loss: 1.4858 - val_acc: 0.0541\n",
            "Epoch 84/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.3832 - acc: 0.1428 - val_loss: 1.4856 - val_acc: 0.0555\n",
            "Epoch 85/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.3790 - acc: 0.1432 - val_loss: 1.4925 - val_acc: 0.0540\n",
            "Epoch 86/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.3765 - acc: 0.1436 - val_loss: 1.4925 - val_acc: 0.0533\n",
            "Epoch 87/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.3720 - acc: 0.1444 - val_loss: 1.4952 - val_acc: 0.0537\n",
            "Epoch 88/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.3666 - acc: 0.1452 - val_loss: 1.4983 - val_acc: 0.0537\n",
            "Epoch 89/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.3617 - acc: 0.1461 - val_loss: 1.5009 - val_acc: 0.0532\n",
            "Epoch 90/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.3579 - acc: 0.1466 - val_loss: 1.5003 - val_acc: 0.0537\n",
            "Epoch 91/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.3539 - acc: 0.1473 - val_loss: 1.5089 - val_acc: 0.0532\n",
            "Epoch 92/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.3502 - acc: 0.1480 - val_loss: 1.5098 - val_acc: 0.0528\n",
            "Epoch 93/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.3464 - acc: 0.1486 - val_loss: 1.5100 - val_acc: 0.0537\n",
            "Epoch 94/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.3427 - acc: 0.1493 - val_loss: 1.5132 - val_acc: 0.0542\n",
            "Epoch 95/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.3394 - acc: 0.1498 - val_loss: 1.5134 - val_acc: 0.0536\n",
            "Epoch 96/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.3356 - acc: 0.1502 - val_loss: 1.5179 - val_acc: 0.0540\n",
            "Epoch 97/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.3314 - acc: 0.1511 - val_loss: 1.5239 - val_acc: 0.0529\n",
            "Epoch 98/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.3290 - acc: 0.1516 - val_loss: 1.5225 - val_acc: 0.0537\n",
            "Epoch 99/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.3257 - acc: 0.1521 - val_loss: 1.5329 - val_acc: 0.0520\n",
            "Epoch 100/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.3218 - acc: 0.1527 - val_loss: 1.5299 - val_acc: 0.0535\n",
            "Epoch 101/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.3183 - acc: 0.1533 - val_loss: 1.5328 - val_acc: 0.0530\n",
            "Epoch 102/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.3150 - acc: 0.1540 - val_loss: 1.5346 - val_acc: 0.0532\n",
            "Epoch 103/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.3131 - acc: 0.1541 - val_loss: 1.5324 - val_acc: 0.0532\n",
            "Epoch 104/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.3095 - acc: 0.1549 - val_loss: 1.5402 - val_acc: 0.0532\n",
            "Epoch 105/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.3059 - acc: 0.1556 - val_loss: 1.5446 - val_acc: 0.0530\n",
            "Epoch 106/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.3028 - acc: 0.1560 - val_loss: 1.5448 - val_acc: 0.0522\n",
            "Epoch 107/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2994 - acc: 0.1565 - val_loss: 1.5514 - val_acc: 0.0533\n",
            "Epoch 108/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2963 - acc: 0.1572 - val_loss: 1.5549 - val_acc: 0.0530\n",
            "Epoch 109/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2939 - acc: 0.1577 - val_loss: 1.5566 - val_acc: 0.0528\n",
            "Epoch 110/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2911 - acc: 0.1582 - val_loss: 1.5596 - val_acc: 0.0532\n",
            "Epoch 111/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2887 - acc: 0.1585 - val_loss: 1.5618 - val_acc: 0.0531\n",
            "Epoch 112/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2856 - acc: 0.1591 - val_loss: 1.5620 - val_acc: 0.0534\n",
            "Epoch 113/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2827 - acc: 0.1598 - val_loss: 1.5636 - val_acc: 0.0530\n",
            "Epoch 114/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2804 - acc: 0.1601 - val_loss: 1.5681 - val_acc: 0.0528\n",
            "Epoch 115/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2779 - acc: 0.1605 - val_loss: 1.5738 - val_acc: 0.0533\n",
            "Epoch 116/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2757 - acc: 0.1610 - val_loss: 1.5725 - val_acc: 0.0529\n",
            "Epoch 117/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2730 - acc: 0.1615 - val_loss: 1.5788 - val_acc: 0.0533\n",
            "Epoch 118/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2701 - acc: 0.1621 - val_loss: 1.5817 - val_acc: 0.0528\n",
            "Epoch 119/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2677 - acc: 0.1624 - val_loss: 1.5789 - val_acc: 0.0533\n",
            "Epoch 120/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2654 - acc: 0.1629 - val_loss: 1.5806 - val_acc: 0.0533\n",
            "Epoch 121/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2631 - acc: 0.1634 - val_loss: 1.5802 - val_acc: 0.0533\n",
            "Epoch 122/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2602 - acc: 0.1639 - val_loss: 1.5833 - val_acc: 0.0533\n",
            "Epoch 123/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2582 - acc: 0.1644 - val_loss: 1.5893 - val_acc: 0.0541\n",
            "Epoch 124/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2557 - acc: 0.1649 - val_loss: 1.5899 - val_acc: 0.0526\n",
            "Epoch 125/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2540 - acc: 0.1651 - val_loss: 1.5936 - val_acc: 0.0530\n",
            "Epoch 126/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2517 - acc: 0.1657 - val_loss: 1.5943 - val_acc: 0.0528\n",
            "Epoch 127/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2490 - acc: 0.1661 - val_loss: 1.5996 - val_acc: 0.0527\n",
            "Epoch 128/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2467 - acc: 0.1666 - val_loss: 1.5995 - val_acc: 0.0533\n",
            "Epoch 129/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2449 - acc: 0.1670 - val_loss: 1.6041 - val_acc: 0.0529\n",
            "Epoch 130/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2432 - acc: 0.1673 - val_loss: 1.6084 - val_acc: 0.0529\n",
            "Epoch 131/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2410 - acc: 0.1678 - val_loss: 1.6062 - val_acc: 0.0525\n",
            "Epoch 132/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2391 - acc: 0.1681 - val_loss: 1.6092 - val_acc: 0.0537\n",
            "Epoch 133/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2373 - acc: 0.1682 - val_loss: 1.6158 - val_acc: 0.0532\n",
            "Epoch 134/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2344 - acc: 0.1691 - val_loss: 1.6116 - val_acc: 0.0533\n",
            "Epoch 135/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2327 - acc: 0.1694 - val_loss: 1.6180 - val_acc: 0.0530\n",
            "Epoch 136/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2311 - acc: 0.1697 - val_loss: 1.6201 - val_acc: 0.0533\n",
            "Epoch 137/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2293 - acc: 0.1701 - val_loss: 1.6191 - val_acc: 0.0533\n",
            "Epoch 138/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2272 - acc: 0.1704 - val_loss: 1.6223 - val_acc: 0.0530\n",
            "Epoch 139/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2250 - acc: 0.1711 - val_loss: 1.6239 - val_acc: 0.0532\n",
            "Epoch 140/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2226 - acc: 0.1714 - val_loss: 1.6221 - val_acc: 0.0531\n",
            "Epoch 141/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2206 - acc: 0.1718 - val_loss: 1.6303 - val_acc: 0.0529\n",
            "Epoch 142/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2199 - acc: 0.1719 - val_loss: 1.6285 - val_acc: 0.0532\n",
            "Epoch 143/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2179 - acc: 0.1724 - val_loss: 1.6294 - val_acc: 0.0530\n",
            "Epoch 144/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2160 - acc: 0.1727 - val_loss: 1.6334 - val_acc: 0.0529\n",
            "Epoch 145/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2146 - acc: 0.1731 - val_loss: 1.6287 - val_acc: 0.0529\n",
            "Epoch 146/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2133 - acc: 0.1731 - val_loss: 1.6354 - val_acc: 0.0533\n",
            "Epoch 147/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2115 - acc: 0.1736 - val_loss: 1.6325 - val_acc: 0.0531\n",
            "Epoch 148/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2094 - acc: 0.1740 - val_loss: 1.6418 - val_acc: 0.0527\n",
            "Epoch 149/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2076 - acc: 0.1744 - val_loss: 1.6395 - val_acc: 0.0525\n",
            "Epoch 150/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2055 - acc: 0.1751 - val_loss: 1.6402 - val_acc: 0.0523\n",
            "Epoch 151/300\n",
            "11673/11673 [==============================] - 21s 2ms/step - loss: 0.2046 - acc: 0.1749 - val_loss: 1.6434 - val_acc: 0.0530\n",
            "Epoch 152/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2028 - acc: 0.1755 - val_loss: 1.6433 - val_acc: 0.0524\n",
            "Epoch 153/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.2016 - acc: 0.1756 - val_loss: 1.6466 - val_acc: 0.0532\n",
            "Epoch 154/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1997 - acc: 0.1760 - val_loss: 1.6460 - val_acc: 0.0521\n",
            "Epoch 155/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1982 - acc: 0.1765 - val_loss: 1.6556 - val_acc: 0.0536\n",
            "Epoch 156/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1974 - acc: 0.1764 - val_loss: 1.6527 - val_acc: 0.0527\n",
            "Epoch 157/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1950 - acc: 0.1771 - val_loss: 1.6516 - val_acc: 0.0530\n",
            "Epoch 158/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1935 - acc: 0.1774 - val_loss: 1.6563 - val_acc: 0.0524\n",
            "Epoch 159/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1918 - acc: 0.1778 - val_loss: 1.6578 - val_acc: 0.0532\n",
            "Epoch 160/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1902 - acc: 0.1782 - val_loss: 1.6581 - val_acc: 0.0532\n",
            "Epoch 161/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1899 - acc: 0.1779 - val_loss: 1.6611 - val_acc: 0.0525\n",
            "Epoch 162/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1885 - acc: 0.1785 - val_loss: 1.6645 - val_acc: 0.0525\n",
            "Epoch 163/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1873 - acc: 0.1785 - val_loss: 1.6633 - val_acc: 0.0529\n",
            "Epoch 164/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1855 - acc: 0.1791 - val_loss: 1.6658 - val_acc: 0.0523\n",
            "Epoch 165/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1835 - acc: 0.1795 - val_loss: 1.6623 - val_acc: 0.0527\n",
            "Epoch 166/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1817 - acc: 0.1799 - val_loss: 1.6676 - val_acc: 0.0525\n",
            "Epoch 167/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1802 - acc: 0.1802 - val_loss: 1.6716 - val_acc: 0.0522\n",
            "Epoch 168/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1792 - acc: 0.1805 - val_loss: 1.6719 - val_acc: 0.0524\n",
            "Epoch 169/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1787 - acc: 0.1805 - val_loss: 1.6690 - val_acc: 0.0521\n",
            "Epoch 170/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1772 - acc: 0.1809 - val_loss: 1.6701 - val_acc: 0.0517\n",
            "Epoch 171/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1760 - acc: 0.1810 - val_loss: 1.6739 - val_acc: 0.0524\n",
            "Epoch 172/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1745 - acc: 0.1814 - val_loss: 1.6743 - val_acc: 0.0527\n",
            "Epoch 173/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1725 - acc: 0.1819 - val_loss: 1.6820 - val_acc: 0.0527\n",
            "Epoch 174/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1714 - acc: 0.1823 - val_loss: 1.6754 - val_acc: 0.0525\n",
            "Epoch 175/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1704 - acc: 0.1822 - val_loss: 1.6815 - val_acc: 0.0529\n",
            "Epoch 176/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1691 - acc: 0.1826 - val_loss: 1.6792 - val_acc: 0.0528\n",
            "Epoch 177/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1673 - acc: 0.1830 - val_loss: 1.6841 - val_acc: 0.0521\n",
            "Epoch 178/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1668 - acc: 0.1832 - val_loss: 1.6817 - val_acc: 0.0528\n",
            "Epoch 179/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1665 - acc: 0.1831 - val_loss: 1.6850 - val_acc: 0.0524\n",
            "Epoch 180/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1650 - acc: 0.1833 - val_loss: 1.6863 - val_acc: 0.0526\n",
            "Epoch 181/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1637 - acc: 0.1837 - val_loss: 1.6914 - val_acc: 0.0530\n",
            "Epoch 182/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1627 - acc: 0.1839 - val_loss: 1.6887 - val_acc: 0.0525\n",
            "Epoch 183/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1615 - acc: 0.1842 - val_loss: 1.6902 - val_acc: 0.0519\n",
            "Epoch 184/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1608 - acc: 0.1843 - val_loss: 1.6963 - val_acc: 0.0524\n",
            "Epoch 185/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1587 - acc: 0.1848 - val_loss: 1.6927 - val_acc: 0.0522\n",
            "Epoch 186/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1574 - acc: 0.1850 - val_loss: 1.6955 - val_acc: 0.0524\n",
            "Epoch 187/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1565 - acc: 0.1855 - val_loss: 1.6981 - val_acc: 0.0520\n",
            "Epoch 188/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1561 - acc: 0.1856 - val_loss: 1.6962 - val_acc: 0.0517\n",
            "Epoch 189/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1551 - acc: 0.1857 - val_loss: 1.6977 - val_acc: 0.0518\n",
            "Epoch 190/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1543 - acc: 0.1857 - val_loss: 1.7030 - val_acc: 0.0522\n",
            "Epoch 191/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1521 - acc: 0.1863 - val_loss: 1.7060 - val_acc: 0.0521\n",
            "Epoch 192/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1512 - acc: 0.1865 - val_loss: 1.7050 - val_acc: 0.0516\n",
            "Epoch 193/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1503 - acc: 0.1867 - val_loss: 1.7066 - val_acc: 0.0512\n",
            "Epoch 194/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1491 - acc: 0.1870 - val_loss: 1.7100 - val_acc: 0.0521\n",
            "Epoch 195/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1480 - acc: 0.1872 - val_loss: 1.7109 - val_acc: 0.0517\n",
            "Epoch 196/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1472 - acc: 0.1873 - val_loss: 1.7126 - val_acc: 0.0514\n",
            "Epoch 197/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1463 - acc: 0.1876 - val_loss: 1.7163 - val_acc: 0.0520\n",
            "Epoch 198/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1451 - acc: 0.1877 - val_loss: 1.7161 - val_acc: 0.0521\n",
            "Epoch 199/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1440 - acc: 0.1881 - val_loss: 1.7155 - val_acc: 0.0517\n",
            "Epoch 200/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1428 - acc: 0.1883 - val_loss: 1.7180 - val_acc: 0.0518\n",
            "Epoch 201/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1419 - acc: 0.1885 - val_loss: 1.7193 - val_acc: 0.0522\n",
            "Epoch 202/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1408 - acc: 0.1887 - val_loss: 1.7186 - val_acc: 0.0524\n",
            "Epoch 203/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1414 - acc: 0.1886 - val_loss: 1.7227 - val_acc: 0.0517\n",
            "Epoch 204/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1405 - acc: 0.1887 - val_loss: 1.7208 - val_acc: 0.0515\n",
            "Epoch 205/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1395 - acc: 0.1889 - val_loss: 1.7247 - val_acc: 0.0519\n",
            "Epoch 206/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1416 - acc: 0.1882 - val_loss: 1.7246 - val_acc: 0.0522\n",
            "Epoch 207/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1387 - acc: 0.1888 - val_loss: 1.7226 - val_acc: 0.0521\n",
            "Epoch 208/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1379 - acc: 0.1893 - val_loss: 1.7316 - val_acc: 0.0518\n",
            "Epoch 209/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1359 - acc: 0.1895 - val_loss: 1.7269 - val_acc: 0.0519\n",
            "Epoch 210/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1348 - acc: 0.1900 - val_loss: 1.7306 - val_acc: 0.0514\n",
            "Epoch 211/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1336 - acc: 0.1902 - val_loss: 1.7363 - val_acc: 0.0520\n",
            "Epoch 212/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1321 - acc: 0.1906 - val_loss: 1.7356 - val_acc: 0.0522\n",
            "Epoch 213/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1311 - acc: 0.1909 - val_loss: 1.7377 - val_acc: 0.0513\n",
            "Epoch 214/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1304 - acc: 0.1911 - val_loss: 1.7394 - val_acc: 0.0517\n",
            "Epoch 215/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1293 - acc: 0.1913 - val_loss: 1.7380 - val_acc: 0.0518\n",
            "Epoch 216/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1286 - acc: 0.1914 - val_loss: 1.7399 - val_acc: 0.0516\n",
            "Epoch 217/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1282 - acc: 0.1916 - val_loss: 1.7450 - val_acc: 0.0517\n",
            "Epoch 218/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1274 - acc: 0.1916 - val_loss: 1.7406 - val_acc: 0.0517\n",
            "Epoch 219/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1270 - acc: 0.1915 - val_loss: 1.7423 - val_acc: 0.0517\n",
            "Epoch 220/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1275 - acc: 0.1915 - val_loss: 1.7470 - val_acc: 0.0518\n",
            "Epoch 221/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1263 - acc: 0.1916 - val_loss: 1.7487 - val_acc: 0.0516\n",
            "Epoch 222/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1246 - acc: 0.1923 - val_loss: 1.7487 - val_acc: 0.0518\n",
            "Epoch 223/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1246 - acc: 0.1921 - val_loss: 1.7517 - val_acc: 0.0512\n",
            "Epoch 224/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1228 - acc: 0.1925 - val_loss: 1.7529 - val_acc: 0.0520\n",
            "Epoch 225/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1218 - acc: 0.1928 - val_loss: 1.7520 - val_acc: 0.0516\n",
            "Epoch 226/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1247 - acc: 0.1918 - val_loss: 1.7570 - val_acc: 0.0515\n",
            "Epoch 227/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1229 - acc: 0.1923 - val_loss: 1.7558 - val_acc: 0.0517\n",
            "Epoch 228/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1208 - acc: 0.1928 - val_loss: 1.7650 - val_acc: 0.0512\n",
            "Epoch 229/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1197 - acc: 0.1932 - val_loss: 1.7661 - val_acc: 0.0514\n",
            "Epoch 230/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1179 - acc: 0.1936 - val_loss: 1.7640 - val_acc: 0.0519\n",
            "Epoch 231/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1169 - acc: 0.1938 - val_loss: 1.7646 - val_acc: 0.0519\n",
            "Epoch 232/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1166 - acc: 0.1940 - val_loss: 1.7678 - val_acc: 0.0512\n",
            "Epoch 233/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1154 - acc: 0.1942 - val_loss: 1.7678 - val_acc: 0.0518\n",
            "Epoch 234/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1148 - acc: 0.1945 - val_loss: 1.7740 - val_acc: 0.0517\n",
            "Epoch 235/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1140 - acc: 0.1945 - val_loss: 1.7703 - val_acc: 0.0513\n",
            "Epoch 236/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1135 - acc: 0.1946 - val_loss: 1.7743 - val_acc: 0.0509\n",
            "Epoch 237/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1132 - acc: 0.1947 - val_loss: 1.7708 - val_acc: 0.0518\n",
            "Epoch 238/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1132 - acc: 0.1945 - val_loss: 1.7761 - val_acc: 0.0515\n",
            "Epoch 239/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1121 - acc: 0.1950 - val_loss: 1.7785 - val_acc: 0.0512\n",
            "Epoch 240/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1115 - acc: 0.1950 - val_loss: 1.7786 - val_acc: 0.0512\n",
            "Epoch 241/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1115 - acc: 0.1949 - val_loss: 1.7765 - val_acc: 0.0511\n",
            "Epoch 242/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1104 - acc: 0.1952 - val_loss: 1.7785 - val_acc: 0.0513\n",
            "Epoch 243/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1096 - acc: 0.1955 - val_loss: 1.7828 - val_acc: 0.0517\n",
            "Epoch 244/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1092 - acc: 0.1953 - val_loss: 1.7840 - val_acc: 0.0512\n",
            "Epoch 245/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1086 - acc: 0.1956 - val_loss: 1.7855 - val_acc: 0.0508\n",
            "Epoch 246/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1074 - acc: 0.1959 - val_loss: 1.7862 - val_acc: 0.0511\n",
            "Epoch 247/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1072 - acc: 0.1958 - val_loss: 1.7925 - val_acc: 0.0509\n",
            "Epoch 248/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1070 - acc: 0.1958 - val_loss: 1.7857 - val_acc: 0.0514\n",
            "Epoch 249/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1061 - acc: 0.1960 - val_loss: 1.7871 - val_acc: 0.0514\n",
            "Epoch 250/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1049 - acc: 0.1963 - val_loss: 1.7873 - val_acc: 0.0512\n",
            "Epoch 251/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1040 - acc: 0.1966 - val_loss: 1.7961 - val_acc: 0.0508\n",
            "Epoch 252/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1034 - acc: 0.1967 - val_loss: 1.7980 - val_acc: 0.0512\n",
            "Epoch 253/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1034 - acc: 0.1967 - val_loss: 1.7988 - val_acc: 0.0508\n",
            "Epoch 254/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1033 - acc: 0.1965 - val_loss: 1.7981 - val_acc: 0.0506\n",
            "Epoch 255/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1027 - acc: 0.1969 - val_loss: 1.7985 - val_acc: 0.0507\n",
            "Epoch 256/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1020 - acc: 0.1969 - val_loss: 1.7985 - val_acc: 0.0508\n",
            "Epoch 257/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.1005 - acc: 0.1973 - val_loss: 1.8035 - val_acc: 0.0503\n",
            "Epoch 258/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0995 - acc: 0.1976 - val_loss: 1.8033 - val_acc: 0.0504\n",
            "Epoch 259/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0996 - acc: 0.1976 - val_loss: 1.8087 - val_acc: 0.0511\n",
            "Epoch 260/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0984 - acc: 0.1978 - val_loss: 1.8098 - val_acc: 0.0509\n",
            "Epoch 261/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0980 - acc: 0.1979 - val_loss: 1.8098 - val_acc: 0.0499\n",
            "Epoch 262/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0980 - acc: 0.1978 - val_loss: 1.8120 - val_acc: 0.0499\n",
            "Epoch 263/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0977 - acc: 0.1980 - val_loss: 1.8111 - val_acc: 0.0498\n",
            "Epoch 264/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0970 - acc: 0.1981 - val_loss: 1.8117 - val_acc: 0.0499\n",
            "Epoch 265/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0969 - acc: 0.1980 - val_loss: 1.8202 - val_acc: 0.0499\n",
            "Epoch 266/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0978 - acc: 0.1976 - val_loss: 1.8141 - val_acc: 0.0511\n",
            "Epoch 267/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0961 - acc: 0.1983 - val_loss: 1.8144 - val_acc: 0.0506\n",
            "Epoch 268/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0946 - acc: 0.1985 - val_loss: 1.8201 - val_acc: 0.0501\n",
            "Epoch 269/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0941 - acc: 0.1988 - val_loss: 1.8211 - val_acc: 0.0513\n",
            "Epoch 270/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0931 - acc: 0.1988 - val_loss: 1.8194 - val_acc: 0.0502\n",
            "Epoch 271/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0928 - acc: 0.1990 - val_loss: 1.8254 - val_acc: 0.0503\n",
            "Epoch 272/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0925 - acc: 0.1991 - val_loss: 1.8225 - val_acc: 0.0506\n",
            "Epoch 273/300\n",
            "11673/11673 [==============================] - 21s 2ms/step - loss: 0.0917 - acc: 0.1992 - val_loss: 1.8221 - val_acc: 0.0508\n",
            "Epoch 274/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0912 - acc: 0.1993 - val_loss: 1.8238 - val_acc: 0.0498\n",
            "Epoch 275/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0907 - acc: 0.1993 - val_loss: 1.8267 - val_acc: 0.0503\n",
            "Epoch 276/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0899 - acc: 0.1997 - val_loss: 1.8308 - val_acc: 0.0502\n",
            "Epoch 277/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0899 - acc: 0.1995 - val_loss: 1.8249 - val_acc: 0.0507\n",
            "Epoch 278/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0900 - acc: 0.1995 - val_loss: 1.8291 - val_acc: 0.0503\n",
            "Epoch 279/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0898 - acc: 0.1995 - val_loss: 1.8343 - val_acc: 0.0503\n",
            "Epoch 280/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0909 - acc: 0.1991 - val_loss: 1.8305 - val_acc: 0.0504\n",
            "Epoch 281/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0895 - acc: 0.1995 - val_loss: 1.8288 - val_acc: 0.0508\n",
            "Epoch 282/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0882 - acc: 0.1999 - val_loss: 1.8366 - val_acc: 0.0504\n",
            "Epoch 283/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0868 - acc: 0.2003 - val_loss: 1.8372 - val_acc: 0.0499\n",
            "Epoch 284/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0856 - acc: 0.2006 - val_loss: 1.8403 - val_acc: 0.0504\n",
            "Epoch 285/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0852 - acc: 0.2007 - val_loss: 1.8387 - val_acc: 0.0502\n",
            "Epoch 286/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0860 - acc: 0.2004 - val_loss: 1.8376 - val_acc: 0.0508\n",
            "Epoch 287/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0851 - acc: 0.2007 - val_loss: 1.8434 - val_acc: 0.0511\n",
            "Epoch 288/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0842 - acc: 0.2009 - val_loss: 1.8453 - val_acc: 0.0507\n",
            "Epoch 289/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0848 - acc: 0.2006 - val_loss: 1.8461 - val_acc: 0.0501\n",
            "Epoch 290/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0843 - acc: 0.2007 - val_loss: 1.8470 - val_acc: 0.0501\n",
            "Epoch 291/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0838 - acc: 0.2009 - val_loss: 1.8457 - val_acc: 0.0500\n",
            "Epoch 292/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0823 - acc: 0.2013 - val_loss: 1.8516 - val_acc: 0.0500\n",
            "Epoch 293/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0833 - acc: 0.2010 - val_loss: 1.8511 - val_acc: 0.0502\n",
            "Epoch 294/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0822 - acc: 0.2014 - val_loss: 1.8542 - val_acc: 0.0505\n",
            "Epoch 295/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0810 - acc: 0.2015 - val_loss: 1.8561 - val_acc: 0.0497\n",
            "Epoch 296/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0804 - acc: 0.2017 - val_loss: 1.8573 - val_acc: 0.0500\n",
            "Epoch 297/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0804 - acc: 0.2017 - val_loss: 1.8577 - val_acc: 0.0501\n",
            "Epoch 298/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0804 - acc: 0.2017 - val_loss: 1.8624 - val_acc: 0.0499\n",
            "Epoch 299/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0803 - acc: 0.2017 - val_loss: 1.8622 - val_acc: 0.0499\n",
            "Epoch 300/300\n",
            "11673/11673 [==============================] - 22s 2ms/step - loss: 0.0798 - acc: 0.2017 - val_loss: 1.8638 - val_acc: 0.0500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3c1b5cc400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R7fE9Vm6Fej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.save('/content/drive/My Drive/NLPProject/mytestmodel')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCILZMI7hyc8",
        "colab_type": "code",
        "outputId": "73c3e61a-a61f-491d-e28f-22e78ea7978e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(encoder_inputs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"input_1:0\", shape=(?, ?), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckH4XOtlh1Bp",
        "colab_type": "code",
        "outputId": "5cb29371-6d79-4483-f3b8-ede5835eb23e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(encoder_states)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 50) dtype=float32>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0A4_ftla88J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lZ0wH1tEnDW",
        "colab_type": "code",
        "outputId": "748da089-27dd-4e55-b71e-b5e33613b8cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "encoder_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, None)              0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, None, 50)          361000    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                [(None, 50), (None, 50),  20200     \n",
            "=================================================================\n",
            "Total params: 381,200\n",
            "Trainable params: 381,200\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTCe9IibE_HH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_state_input_h = Input(shape=(50,))\n",
        "decoder_state_input_c = Input(shape=(50,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "final_dex2= dex(decoder_inputs)\n",
        "\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dex2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2)\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nnzv-5TGFJze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = target_token_index['START_']\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += ' '+sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '_END' or\n",
        "           len(decoded_sentence) > 52):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00jHetRmFRly",
        "colab_type": "code",
        "outputId": "c252fef6-48ba-4f27-b035-9ddfc049d17c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 897
        }
      },
      "source": [
        "#print(len(encoder_input_data))\n",
        "#for seq_index in [1407,2012,4003,4006, 4005, 4006, 4009, 4009, 4010, 4011, 4013, 4016, 4015, 4153]:\n",
        "#all_key_words=set()\n",
        "#all_ques_words=set()\n",
        "with open('/content/drive/My Drive/NLPProject/TEST.txt','r') as testmodel:\n",
        "  line = testmodel.readline()\n",
        "  count = 1\n",
        "  totalkeywords = []\n",
        "  while line:\n",
        "    words = [word.strip().lower() for word in line.split(' ')]\n",
        "    print(words)\n",
        "    answer  = words[-1]\n",
        "    keywords = \" \".join(words[:-1])\n",
        "    print(answer,\" keyword:\",keywords)\n",
        "    totalkeywords.append([keywords,answer])\n",
        "    count +=1\n",
        "    if count == 100:\n",
        "      break\n",
        "    line = testmodel.readline()\n",
        "# for k in totalkeywords:\n",
        "#     for word in k[0].split():\n",
        "#         if word not in all_test_key_words:\n",
        "#             all_test_key_words.add(word)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ticking', 'sound', 'alternator', 'starter', 'battery', 'starter']\n",
            "starter  keyword: ticking sound alternator starter battery\n",
            "['measurement', 'is', 'speed', 'wind', 'measured', 'knots']\n",
            "knots  keyword: measurement is speed wind measured\n",
            "['jim', 'morrison', 'died', 'yes']\n",
            "yes  keyword: jim morrison died\n",
            "['mexico', 'bigger', 'austraila', 'yes']\n",
            "yes  keyword: mexico bigger austraila\n",
            "['teachings', 'muslim', 'influenced', 'arts', 'artisticstyles']\n",
            "artisticstyles  keyword: teachings muslim influenced arts\n",
            "['bacterial', 'infections', 'cause', 'cancer', 'no']\n",
            "no  keyword: bacterial infections cause cancer\n",
            "['man', 'loves', 'woman', 'depends']\n",
            "depends  keyword: man loves woman\n",
            "['solar', 'energy', 'affect', 'weather', 'climate', 'heavyrainfall']\n",
            "heavyrainfall  keyword: solar energy affect weather climate\n",
            "['multiplex', 'control', 'unit', 'replace', 'honda', 'accord', 'reporgramming']\n",
            "reporgramming  keyword: multiplex control unit replace honda accord\n",
            "['smoking', 'affect', 'lung', 'capacity', 'buildmucus']\n",
            "buildmucus  keyword: smoking affect lung capacity\n",
            "['boys', 'pulse', 'rate', 'girls', 'no']\n",
            "no  keyword: boys pulse rate girls\n",
            "['ad', 'time', 'period', 'years', 'bc', 'time', 'period', '0']\n",
            "0  keyword: ad time period years bc time period\n",
            "['nipples', 'cramping', 'ovulation', 'pregnant']\n",
            "pregnant  keyword: nipples cramping ovulation\n",
            "['element', 'treating', 'cancer', 'technetium']\n",
            "technetium  keyword: element treating cancer\n",
            "['sharpie', 'is', 'poisonous', 'no']\n",
            "no  keyword: sharpie is poisonous\n",
            "['earths', 'atmosphere', 'layer', 'weather', 'occur', 'troposphere']\n",
            "troposphere  keyword: earths atmosphere layer weather occur\n",
            "['marriage', 'story', 'amador', 'dagio', 'wedding']\n",
            "wedding  keyword: marriage story amador dagio\n",
            "['father', 'amador', 'dagio', 'fermindaguio']\n",
            "fermindaguio  keyword: father amador dagio\n",
            "['distance', 'measured', 'east', 'west', 'longitude']\n",
            "longitude  keyword: distance measured east west\n",
            "['surface', 'area', 'speed', 'chemical', 'reaction', 'directlypropositional']\n",
            "directlypropositional  keyword: surface area speed chemical reaction\n",
            "['ways', 'speed', 'chemical', 'reaction', 'three']\n",
            "three  keyword: ways speed chemical reaction\n",
            "['drugs', 'similar', 'percocet', 'vicodin']\n",
            "vicodin  keyword: drugs similar percocet\n",
            "['girls', 'hump', 'dogs', 'no']\n",
            "no  keyword: girls hump dogs\n",
            "['search', 'engines', 'invented', 'google', 'larrypage']\n",
            "larrypage  keyword: search engines invented google\n",
            "['molly', 'pitcher', 'parent', 'name', 'johngeorgludwick']\n",
            "johngeorgludwick  keyword: molly pitcher parent name\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZko0cGK3SKu",
        "colab_type": "code",
        "outputId": "90b801d2-c7e9-46d4-fcc0-73cd0891ce66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "print(encoder_input_data)\n",
        "print(encoder_input_data[2001: 2001 + 1])\n",
        "print(encoder_input_data[2001])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5432.  553. 3514. ...    0.    0.    0.]\n",
            " [5432.  553. 6912. ...    0.    0.    0.]\n",
            " [5432.  553. 6912. ...    0.    0.    0.]\n",
            " ...\n",
            " [1909. 4214. 2061. ...    0.    0.    0.]\n",
            " [1909. 4214. 2061. ...    0.    0.    0.]\n",
            " [1909. 5283. 2061. ...    0.    0.    0.]]\n",
            "[[ 338. 7218. 5898.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            "     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.]]\n",
            "[ 338. 7218. 5898.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D2FvM-Z0AK1",
        "colab_type": "code",
        "outputId": "fdd68377-91f6-4a16-aa14-8cad1326942e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "f = open('GeneratedQAOutput.txt','w')\n",
        "encoder_test_input_data = np.zeros((1, maxkeylength),dtype='float32')\n",
        "for wordseq in totalkeywords:\n",
        "    input_seq = wordseq[0]\n",
        "    for t, word in enumerate(input_seq.split()):\n",
        "        encoder_test_input_data[0, t] = input_token_index[word]\n",
        "    decoded_sentence = decode_sequence(encoder_test_input_data[0:1])\n",
        "    print('-', file=f)\n",
        "    print('Input sentence:', input_seq, file=f)\n",
        "    print('Decoded sentence:', decoded_sentence, file=f)\n",
        "    print('Anwser:',wordseq[1], file=f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: ticking sound alternator starter battery\n",
            "Decoded sentence:  what to human part of the the battery is the battery\n",
            "Anwser: starter\n",
            "-\n",
            "Input sentence: measurement is speed wind measured\n",
            "Decoded sentence:  what measurement is wind speed measured east and in parallelwhat\n",
            "Anwser: knots\n",
            "-\n",
            "Input sentence: jim morrison died\n",
            "Decoded sentence:  jim jordans baseball what the s _END\n",
            "Anwser: yes\n",
            "-\n",
            "Input sentence: mexico bigger austraila\n",
            "Decoded sentence:  how much can chemistry be considered in water _END\n",
            "Anwser: yes\n",
            "-\n",
            "Input sentence: teachings muslim influenced arts\n",
            "Decoded sentence:  cool quad procedures of flower in a and life in spain\n",
            "Anwser: artisticstyles\n",
            "-\n",
            "Input sentence: bacterial infections cause cancer\n",
            "Decoded sentence:  how chemistry is a creme brulee in _END\n",
            "Anwser: no\n",
            "-\n",
            "Input sentence: man loves woman\n",
            "Decoded sentence:  about a soldier wear for a _END\n",
            "Anwser: depends\n",
            "-\n",
            "Input sentence: solar energy affect weather climate\n",
            "Decoded sentence:  how much will a great dane a cool contest _END\n",
            "Anwser: heavyrainfall\n",
            "-\n",
            "Input sentence: multiplex control unit replace honda accord\n",
            "Decoded sentence:  having vin for ford expedition _END\n",
            "Anwser: reporgramming\n",
            "-\n",
            "Input sentence: smoking affect lung capacity\n",
            "Decoded sentence:  how many smoking on the lower left in the lower right\n",
            "Anwser: buildmucus\n",
            "-\n",
            "Input sentence: boys pulse rate girls\n",
            "Decoded sentence:  honda ex go _END\n",
            "Anwser: no\n",
            "-\n",
            "Input sentence: ad time period years bc time period\n",
            "Decoded sentence:  how long can a japanese revere have his first nba black\n",
            "Anwser: 0\n",
            "-\n",
            "Input sentence: nipples cramping ovulation\n",
            "Decoded sentence:  cramping after ovulation is back from east to west _END\n",
            "Anwser: pregnant\n",
            "-\n",
            "Input sentence: element treating cancer\n",
            "Decoded sentence:  what would the most important invention in the life in\n",
            "Anwser: technetium\n",
            "-\n",
            "Input sentence: sharpie is poisonous\n",
            "Decoded sentence:  what is the range of a cat is pele in water on a colon\n",
            "Anwser: no\n",
            "-\n",
            "Input sentence: earths atmosphere layer weather occur\n",
            "Decoded sentence:  why can be get energy with the industry in _END\n",
            "Anwser: troposphere\n",
            "-\n",
            "Input sentence: marriage story amador dagio\n",
            "Decoded sentence:  which roof has anyone play golf on _END\n",
            "Anwser: wedding\n",
            "-\n",
            "Input sentence: father amador dagio\n",
            "Decoded sentence:  who sings liberal group in mother in land _END\n",
            "Anwser: fermindaguio\n",
            "-\n",
            "Input sentence: distance measured east west\n",
            "Decoded sentence:  what changes made protein quality or in a car yard in\n",
            "Anwser: longitude\n",
            "-\n",
            "Input sentence: surface area speed chemical reaction\n",
            "Decoded sentence:  what changes use good in an object raise your hdl level\n",
            "Anwser: directlypropositional\n",
            "-\n",
            "Input sentence: ways speed chemical reaction\n",
            "Decoded sentence:  will a fine powder used for making cakes _END\n",
            "Anwser: three\n",
            "-\n",
            "Input sentence: drugs similar percocet\n",
            "Decoded sentence:  which can a olympic games in football game drug test\n",
            "Anwser: vicodin\n",
            "-\n",
            "Input sentence: girls hump dogs\n",
            "Decoded sentence:  can japanse psp game can live with psp can trainer and\n",
            "Anwser: no\n",
            "-\n",
            "Input sentence: search engines invented google\n",
            "Decoded sentence:  who sings marmaduke the mother of wwe long can a slider\n",
            "Anwser: larrypage\n",
            "-\n",
            "Input sentence: molly pitcher parent name\n",
            "Decoded sentence:  in the characteristic unit of nazarene and southern germany\n",
            "Anwser: johngeorgludwick\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GumUp-NWT6M",
        "colab_type": "text"
      },
      "source": [
        "### BLEU Score "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isK25rKPFWje",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a7df5227-cefc-4893-f2e2-971926d29e9a"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "f = open('BleuScores.txt','w')\n",
        "bleu_score = 0\n",
        "input_tag = 'Input sentence:'\n",
        "output_tag = 'Decoded sentence:'\n",
        "with open('/content/GeneratedQAOutput.txt','r+',encoding='utf-8') as outputFile:\n",
        "  num = 0\n",
        "  score = 0\n",
        "  sf = SmoothingFunction()\n",
        "  for line in outputFile:\n",
        "    if '-' in line:\n",
        "      num += 1\n",
        "      for i in range(3):\n",
        "        try:\n",
        "          line = next(outputFile)\n",
        "          if input_tag in line:\n",
        "            input_sequence = (line.replace(input_tag,'')).strip()\n",
        "          elif output_tag in line:\n",
        "            decoded_sentence = (line.replace(output_tag,'')).strip()\n",
        "        except StopIteration:\n",
        "          break\n",
        "      score = sentence_bleu(input_sequence, decoded_sentence,smoothing_function = sf.method7)\n",
        "      print('-', file=f)\n",
        "      print('Input sentence:', input_sequence, file=f)\n",
        "      print('Decoded sentence:', decoded_sentence, file=f)\n",
        "      print('Bleu score : ',score, file=f)\n",
        "      bleu_score += score\n",
        "print(\"Cummulative Bleu Score obtained is : \",bleu_score/num)\n",
        "f.close()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cummulative Bleu Score obtained is :  0.3687531437076924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA5C36CAYNmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}